server:
 servlet:
   contextPath: '/api/v1'

entrada:
  input:
    delete-after-read: false
    # inputstream buffer size (in kb)
    buffer: 64
    # if object is claimed by worker but not processed before (ts_start + max-proc-time) then release object
    max-proc-time: 30
  nameserver:
    default-name: unknown-svr
    default-site: unknown-site
  parquet:
   file:
     # max Parquet filesize (mb)
     max-size: 256
  process:
    # if worker time processing pcap > x min, it is marked as stalled
    stalled: 10
  schedule:
    # update reference data every x min
    updater: 60
    # check for stalled worker every x min
    liveness: 1
    # check for expired oject ever x min
    expired-object: 5
  security:
    # token used for access to REST API (use X-API-KEY HTTP header)
    token: '94591089610224297274859827590711'
  privacy:
    # when enabled the IP addresses are not written to parquet file
    enabled: false
  directory:
    # s3 directory/prefix for pcap files and reference data
    pcap: 'pcap'
    reference: 'reference'
  s3:
    # use default aws env vars so aws libs automatically get correct credentials
    access-key: '${AWS_ACCESS_KEY_ID}'
    secret-key: '${AWS_SECRET_ACCESS_KEY}'
    region: ${AWS_REGION}'
    # only use endpoint when not using aws 
    endpoint:
    bucket: 'sidnlabs-iceberg-data'
    pcap-directory: 'pcap'
  messaging:
    # names of messaging queues
    request:
      name: 'entrada-s3-event'
      retention: 86400
      visibility-timeout: 600
    command:
      name: 'entrada-command'
      retention: 60
      visibility-timeout: 1
    leader:
      name: 'entrada-leader'
      retention: 86400
      visibility-timeout: 300
  # set leader to true for 1 container when using non k8s deployment
  leader: false
  metrics:
    # write metics to influxdb
    enabled: true
    influxdb:
      org: SIDN
      bucket: entrada
      token: 
      url:
      env: test
  provisioning:
    # auto create all required componemts such as bucket and queues
    # might be useful when entrada2 application does not have these permissions on aws.
    enabled: true
      

spring:
 main:
   # workaround, do not remove
   allow-circular-references: true
 application: 
   # name of container/pod
   name: 'entrada2'
 rabbitmq:
    host:
    password:
    username:
    retry-attempts: 3
    backoff-interval: 1000
    backoff-multiplier: 2
    backoff-max-interval: 5000
    concurrent-consumers: 1
    max-concurrent-consumers: 1
 cloud:
    bootstrap:
      enabled: false
    kubernetes:
      # see: https://docs.spring.io/spring-cloud-kubernetes/reference/appendix.html
      leader:
        # name of k8s configmap to use to write leader pod name to
        config-map-name: entrada-leader
        role: master 
    aws:
      sqs:
        # when using aws, enable sqs for messaging
        enabled: false
    loadbalancer:
      enabled: false
    openfeign:
      client:
        config:
          default:
            connectTimeout: 5000
            readTimeout: 5000
            loggerLevel: basic
          maxmindClient:
            url: 'https://download.maxmind.com/app/geoip_download?suffix=tar.gz'
            dismiss404: false

#logging:
#  level:
#    root: DEBUG
   
#logging:
#  level:
#    nl.sidn.entrada2: DEBUG
 
iceberg:
  # only use catalog when using the REST catalog server, not when using aws Glue
  catalog:
    host:
    port: 5432
    name:
    user:
    password: 
  warehouse-dir: 'warehouse'
  # see iceberg docs for: write.parquet.compression-codec
  compression: 'ZSTD'
  metadata:
    version:
      # see iceberg docs for: write.metadata.previous-versions-max
      max: 100
  table:
    namespace: 'entrada2'
    name: 'dns'
    location: 's3://${entrada.s3.bucket}/${iceberg.table.namespace}'
    # sorting allows for better compression but increases resource(cpu/ram) usage
    sorted: true
  parquet:
    # keep dictionary size low, to force parquet to use bloomfilter when bloomfilter is enabled
    # but also do not use a to low number of bytes or none of the columns will be able to use
    # dictionary encoding.
    # see iceberg docs for: write.parquet.dict-size-bytes
    dictionary-max-bytes: 102400 #10240
    # see iceberg docs for: write.parquet.page-row-limit
    page-limit: 20000
    # see iceberg docs for: write.parquet.bloom-filter-enabled.column.dns_domainname
    # using bloomfilter may result in larger data files but query execution performance may improve
    bloomfilter: true

  

#########################
#        Resolvers      #
#########################

# max size of the "IP address to resolver" match cache
# keep cache because checking IP address is expensive
# but must make sure not to use a giant cache and get out-of-memory error
resolver:
  match:
    cache:
      size: 10000
  google:
    hostname: 'locations.publicdns.goog.'
    timeout: 15
  opendns:
    url: 'https://umbrella.cisco.com/why-umbrella/global-network-and-traffic'
    timeout: 15
  cloudflare:
    url:
      v4: 'https://www.cloudflare.com/ips-v4'
      v6: 'https://www.cloudflare.com/ips-v6'
      timeout: 15
     

  
#########################
#      MaxMind GeoIP    #
#########################
maxmind:
 max-age: 24
 license:
   free:
   paid:
 country:
   free: 'GeoLite2-Country'
   paid: 'GeoIP2-Country'
 asn:
   free: 'GeoLite2-ASN'
   paid: 'GeoIP2-ISP'
     

management:
  influx.metrics.export:
    enabled: false
  endpoints:
    web:
      exposure:
        include: 'health,info,prometheus'
  health:
    livenessState:
      enabled: true
    readinessState:
      enabled: true    
  endpoint:
    health:
      enabled: true
      probes:
        enabled: true
    heapdump:
      enabled: false
    logfile:
      enabled: false
       
       

