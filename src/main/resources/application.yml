entrada:
  nameserver:
    default-name: unknown-ns
    default-site: unknown-site
  object:
    # after wait expire mark as not picked up and send to queue again
    max-wait-time-secs: 3600
    # mark as failed after max proc time expired, high load might cause longer proc times.
    # do not set too low
    max-proc-time-secs: 7200
  process:
    # max time for pcap file to be procssed in min
    # if worker is processing pcap longer than x secs, it is marked as stalled
    max-proc-time-secs: 600
  schedule:
    # update reference data every x min
    updater-min: 120
    # check for stalled worker every x min
    liveness-min: 1
    # check for expired oject every x min
    expired-object-min: 10
  security:
    # token used for access to REST API (use X-API-KEY HTTP header) and actuator endpoints
    # this is a test token, and it being here IS NOT as security issue
    # WARNING: change this test token to new secure token for use in production environment
    token: '94591089610224297274859827590711'
  privacy:
    # when enabled the IP addresses are not written to parquet file
    enabled: false
  s3:
    # use default aws env vars so aws libs automatically get correct credentials
    access-key: '${AWS_ACCESS_KEY_ID}'
    secret-key: '${AWS_SECRET_ACCESS_KEY}'
    region: ${AWS_REGION}'
    # only use endpoint when not using aws 
    endpoint:
    bucket: sidnlabs-iceberg-data
    # dir for new pcaps
    pcap-in-dir: pcap-in
    # optionally move processed files to a different directory, better for detecting failed objects and aws object expiration based on age
    # pcaps are deleted after procesessing when pcap-done-dir is left empty
    pcap-done-dir: pcap-done
    # files that could not be processed correctly are moved to the "failed" directory
    pcap-failed-dir: pcap-failed
    # location for reference data
    reference-dir: reference
    # location of iceberg data files
    warehouse-dir: database
  messaging:
    # names of messaging queues
    request:
      # lifecycle events from s3 bucket are sent to this queue
      name: 'entrada-s3-event'
      aws:
        retention: 86400
        visibility-timeout: 600
    command:
      # commands are sent to all instances listening on this queue
      name: 'entrada-command'
      aws:
        retention: 60
        visibility-timeout: 1
    leader:
      # the leader will listen to this queue
      name: 'entrada-leader'
      aws:
        retention: 86400
        visibility-timeout: 300
  # set leader to true for 1 container when using non k8s deployment
  leader: false
  provisioning:
    # auto create all required components such as the bucket and queues
    # might be useful when entrada2 application does not have these permissions on aws.
    enabled: true


server:
  servlet:
    context-path: /api/v1
      

spring:
 main:
   # workaround, do not remove
   allow-circular-references: true
 application: 
   # name of container/pod
   name: 'entrada2'
 rabbitmq:
    host:
    password:
    username:
    retry-attempts: 5
    backoff-interval: 1000
    backoff-multiplier: 2
    backoff-max-interval: 5000
 cloud:
    bootstrap:
      enabled: false
    kubernetes:
      # see: https://docs.spring.io/spring-cloud-kubernetes/reference/appendix.html
      leader:
        # name of k8s configmap to use to write leader pod name to
        config-map-name: entrada-leader
        role: master 
    aws:
      sqs:
        # when using aws, enable sqs for messaging
        enabled: false
    loadbalancer:
      # we do not use the client loadbalancer
      enabled: false
    openfeign:
      client:
        config:
          default:
            connectTimeout: 5000
            readTimeout: 5000
            loggerLevel: basic
          maxmindClient:
            url: 'https://download.maxmind.com/app/geoip_download?suffix=tar.gz'
            dismiss404: false

 
iceberg:
  # JDBC catalog, use this when not using aws Glue catalog
  # only Postgresql is supported
  catalog:
    host:
    port: 5432
    name:
    user:
    password: 
    warehouse-location: 's3://${entrada.s3.bucket}/${entrada.s3.warehouse-dir}'
  # see iceberg docs for: write.parquet.compression-codec
  compression: 'ZSTD'
  metadata:
    version:
      # see iceberg docs for: write.metadata.previous-versions-max
      max: 100
  table:
    name: 'dns'
    namespace: 'entrada2'
    # directory for Iceberg table data
    location: 's3://${entrada.s3.bucket}/${entrada.s3.warehouse-dir}/${iceberg.table.namespace}/${iceberg.table.name}'
    # sorting allows for better compression but increases resource(cpu/ram) usage
    sorted: true
  parquet:
    # keep dictionary size low, to force parquet to use bloomfilter when bloomfilter is enabled
    # but also do not use a to low number of bytes or none of the columns will be able to use
    # dictionary encoding.
    # see iceberg docs for: write.parquet.dict-size-bytes
    dictionary-max-bytes: 102400 #10240
    # see iceberg docs for: write.parquet.page-row-limit
    page-limit: 20000
    # see iceberg docs for: write.parquet.bloom-filter-enabled.column.dns_domainname
    # using bloomfilter may result in larger data files but query execution performance may improve
    bloomfilter: true
    # max Parquet filesize (mb)
    max-size: 256

  

#########################
#        Resolvers      #
#########################

resolver:
  match:
    cache:
      # max size of the "IP address to resolver" match cache
      # keep cache because checking IP address is expensive
      # but must make sure not to use a giant cache and get out-of-memory error
      size: 10000
  google:
    hostname: 'locations.publicdns.goog.'
    timeout: 15
  opendns:
    url: 'https://umbrella.cisco.com/why-umbrella/global-network-and-traffic'
    timeout: 15
  cloudflare:
    url:
      v4: 'https://www.cloudflare.com/ips-v4'
      v6: 'https://www.cloudflare.com/ips-v6'
      timeout: 15
     

  
#########################
#      MaxMind GeoIP    #
#########################
maxmind:
 # max age of local copy of MM db before downloading update ( hours)
 max-age-hr: 24
 license:
   free:
   paid:
 country:
   free: 'GeoLite2-Country'
   paid: 'GeoIP2-Country'
 asn:
   free: 'GeoLite2-ASN'
   paid: 'GeoIP2-ISP'
     

management:
  influx.metrics.export:
    enabled: true
    bucket: entrada2
    org: SIDN
    token:
    uri:
    step: 1m
  # do not send standard metrics such as for jvm to influxdb
  # set to true if you would like this anyway
  metrics:
    enable: 
      all: false
      pcap: true
  endpoints:
    enabled-by-default: false 
    web:
      exposure:
        include: health,info,prometheus
  health:
    livenessState:
      enabled: true
    readinessState:
      enabled: true    
  endpoint:
    health:
      enabled: true
      probes:
        enabled: true
       

