# ENTRADA2

A scalable tool for converting DNS data to database table based on [Apache Iceberg](https://iceberg.apache.org/) and [Apache Parquet](https://parquet.apache.org/) data.   

# ENTRADA2 vs ENTRADA
ENTRADA2 is designed to be more performent, scalable and to reduce the output size of the generated Parquet data.
Due to these changes, data generated by ENTRADA2 uses a different table schema and is NOT compatible with the original ENTRADA table schema.

List of changes:

- Remove Hadoop dependencies
- Add support for Kubernetes
- Add support for Apache Iceberg
- The qname column now only contains the labels preceding the domainname
- Rows are now sorted by domainname for better gzip compression
- Default column compression changed to gzip, not snappy
- Use small Parquet max dictionary size, to prevent domainname column using dict
- Use bloomfilter for domainname column, performance boost when filtering on domainname 
- Renamed table columns
- Removed unused tables columns
- Each Docker container can handle any pcap, not limited to configured server anymore
- Upload of pcap file to S3 triggers ENTRADA2 processing via messaging events


ENTRADA2 supports the following deployment modes:
- Docker
- Kubernetes
- AWS cloud

ENTRADA2 is based on the following components:  

- S3 storage (MinIO, AWS)
- Messaging Queue ( RabbitMQ, AWS SQS/SNS)
- Metadata Database (PostgreSQL)
- Metrics (InfluxDB)
- Query engine (Trino, AWS Athena, Spark)
- [REST based Iceberg catalog server](https://github.com/SIDN/iceberg-rest-catalog-server)

# Build

```
mvn package
docker build --tag=sidnlabs/entrada2:0.0.1 .
docker push sidnlabs/entrada2:0.0.1
```

# Getting started

If you want to get started testing ENTRADA2, an easy method is by using the Docker Compose script to create a test
environment containing all the required components and using default configuration settings.
ENTRADA2 uses the [Maxmind](https://www.maxmind.com) GeoIP2 database, if you have a license then set the MAXMIND_LICENSE_PAID
environment variable, otherwise signup for the free [GeoLite2](https://dev.maxmind.com/geoip/geolite2-free-geolocation-data ) database and use 
the MAXMIND_LICENSE_FREE environment variable.

Example using the Maxmind GeoIP2 database:

```
export MAXMIND_LICENSE_PAID=<your-key-here>
docker-compose --profile test up
```


## Uploading pcap file
When all components have started, you can upload a pcap file to s3 and processing of the file will start automatically.  
Use the the following tags when uploading file to S3:

- entrada-ns-server: Logical name of the name server
- entrada-ns-anycast-site: Anycast site of the name server

Example using MinIO:  

```
mc cp --tags "entrada-ns-server=ns1.example.nl&entrada-ns-anycast-site=ams"  \
ams-ns1-150_2023-10-04-11:09:51.pcap.gz minio/sidnlabs-iceberg-data/pcap/ams-ns1-150_2023-10-04-11:09:51.pcap.gz
```


## Analysing results
The results may be analysed using different tools, such as AWS Athena, Trino or Apache Spark. 
The Docker Compose script automaticlly starts Trino, for quickly analysing a limited dataset.  

Start the Trino client (installed in  Trino container):

```
docker exec -it docker-trino-1 trino
```

Switch to the correct catalog:

```
use iceberg_entrada.entrada2
```

Query data in dns table:

```
select count(1) from dns;
```

## Cleanup
To cleanup the test evironment, stop the Docker containers, delete the Docker volumes and restart the containers.

```
docker system prune -f
docker volume rm docker_dataVolume
docker volume rm docker_pgVolume
docker-compose --profile test up
```


## API


## Running multiple containers



## Components UI
Some of the components provide a web interface, below are the URLs for the components started by the docker compose script.
Login credentials can be found in the script.

- [MinIO](http://localhost:9000)
- [RabbitMQ](http://localhost:15672/)
- [InfluxDB](http://localhost:8086/)
- [Trino](http://localhost:8085/) 
